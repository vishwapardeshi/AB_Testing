{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Early_Stopping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pklpma_qEzlB",
        "colab_type": "text"
      },
      "source": [
        "# A/B Test: Early Stopping\n",
        "\n",
        "If you peek at the results of an experiment before data collection is complete, and choose to stop early because the test is showing statistical significance, you run the risk of a significant increase in your Type I error rate: believing that your experiment had an effect, when in fact it did not. \n",
        "\n",
        "In this notebook, it will be demonstrated that for an experiment based off of a single traditional statistical test, doing a single peek halfway through the run-time will increase a base Type I error rate from 5% to about 8.6%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-q71f9GENjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHKk2AkWFH4c",
        "colab_type": "text"
      },
      "source": [
        "The simulation function below uses a bernoulli / binomial success model for the outcome metric, measuring against a historic baseline. That is, each observation is a single coin flip with success probability \"p\". \n",
        "\n",
        "If we see a number of successes that is unusual for our baseline value of \"p\", then we declare a statistically significant result. \n",
        "\n",
        "We will divide the experiment length into multiple 'blocks', checking the status of the experiment after each block is complete. Our outputs of interest are the proportion of trials that are statistically significant in any test, and the proportion of trials that are statistically significant after each individual block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlxR5NYDFKs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def peeking_sim(alpha = .05, p = .5, n_trials = 1000, n_blocks = 2, n_sims = 10000):\n",
        "    \"\"\"\n",
        "    This function simulates the rate of Type I errors made if an early\n",
        "    stopping decision is made based on a significant result when peeking ahead.\n",
        "    \n",
        "    Input parameters:\n",
        "        alpha: Supposed Type I error rate\n",
        "        p: Probability of individual trial success\n",
        "        n_trials: Number of trials in a full experiment\n",
        "        n_blocks: Number of times data is looked at (including end)\n",
        "        n_sims: Number of simulated experiments run\n",
        "        \n",
        "    Return:\n",
        "        p_sig_any: Proportion of simulations significant at any check point, \n",
        "        p_sig_each: Proportion of simulations significant at each check point\n",
        "    \"\"\"\n",
        "    \n",
        "    # generate data\n",
        "    trials_per_block = np.ceil(n_trials / n_blocks).astype(int)\n",
        "    data = np.random.binomial(trials_per_block, p, [n_sims, n_blocks])\n",
        "    \n",
        "    # standardize data\n",
        "    data_cum_sum = np.cumsum(data, axis = 1)\n",
        "    block_sizes = trials_per_block * np.arange(1, n_blocks+1, 1)\n",
        "    block_means = block_sizes * p\n",
        "    block_sds   = np.sqrt(block_sizes * p * (1-p))\n",
        "    data_zscores = (data_cum_sum - block_means) / block_sds\n",
        "    \n",
        "    # test outcomes\n",
        "    z_crit = stats.norm.ppf(1-alpha/2)\n",
        "    sig_flags = np.abs(data_zscores) > z_crit\n",
        "    p_sig_any = (sig_flags.sum(axis = 1) > 0).mean()\n",
        "    p_sig_each = sig_flags.mean(axis = 0)\n",
        "    \n",
        "    return (p_sig_any, p_sig_each)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMvOh-KKLAy-",
        "colab_type": "text"
      },
      "source": [
        "The above function does the following three:\n",
        "\n",
        "**1. Simulate some data**\n",
        "* Compute the number of trials per block. For simplicity, just round up any fractions so that each block has the same number of trials: we might end up with slightly more trials per block than the corresponding function parameter.\n",
        "* Generate a data matrix with the number of successes observed in each block: the number of rows should be the number of simulations and the number of columns the number of blocks. You can do this with a single call to numpy's random.binomial function.\n",
        "\n",
        "**2. Compute z-scores at each 'peek'**\n",
        "* For each row, compute the cumulative number of successes after each 'block' of the experiment using numpy's cumsum function. The result should be a matrix with the same dimensions as the data, but each column cumulatively adds up the values in each row up to that point.\n",
        "* Compute the expected mean and standard deviation for the number of successes after each 'block' of the experiment. Remember that this will be based on the binomial distribution and is centered on the raw counts, rather than proportion of successes. It'll be useful to create a vector with the cumulative sum of trials after each block to facilitate these calculations.\n",
        "* Use the cumulative counts, the expected counts, and the standard deviations, to compute the z-scores for each peek at the experiment.\n",
        "\n",
        "**3. Aggregate test outcomes**\n",
        "* Compute a critical z-value using the supposed Type I error rate. Use this critical value to flag which of the z-scores would be counted as statistically significant, and which would not.\n",
        "* The proportion of trials that are significant at any test will be the proportion of rows that have at least one flagged value. The proportion of trials that are significant at each block will be the mean number of flagged values in each column; this will be a 1-d array. Return both of these values as the output of the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hykOpvQ2GNpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "1ad70b77-bbf2-4e6d-f9ef-35e996aba981"
      },
      "source": [
        "peeking_sim(n_trials = 10_000, n_sims = 100_000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.08434, array([0.04983, 0.05174]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE6zEPqELjud",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> Running the function on the default parameters as given should return a tuple of results where the probability of any significant test outcome across the two blocks is around 8.6% and the probability of a significant test outcome at each individual block checkpoint is around 5%. Increase the number of trials and number of simulations to get more accurate estimates. You should also see how the overall Type I error rate increases with additional peeks!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZbmeddVLsmi",
        "colab_type": "text"
      },
      "source": [
        "# Multi Comparison Approach to Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7me3A0QL0B7",
        "colab_type": "text"
      },
      "source": [
        "The safest way we could deal with performing multiple checks and making poor early stopping decisions is to simply not do it. Once an experiment has been planned and all assignment procedures checked, you should just let the experiment run to completion and just assess the results at the very end. That's not to say that you can't perform early stopping, but it does require additional planning.\n",
        "\n",
        "One way in which you could solve for multiple peeking is to adjust the significance level of individual tests so that the overall error rate is at its desired level. But applying the Bonferroni corrections as shown earlier in the lesson will definitely be too conservative, since we know that there is a correlation in test results between peeks. If we see some simulated run with z-score above the threshold at the halfway point, it's more likely to be above that threshold at the end point, compared to some other simulated run that is not statistically significant at the halfway point. \n",
        "\n",
        "One way in which we can obtain a better significance threshold is through the power of simulation. After performing the same steps 1 and 2 above, we want to find a significance level that would call our desired proportion of simulated tests as statistically significant:\n",
        "\n",
        "1. Simulate some data (as above)\n",
        "2. Compute z-scores at each 'peek' (as above)\n",
        "3. Obtain required individual-test error rate\n",
        "  * A run is considered statistically significant if it exceeds the critical bounds at any peek. Obtain the maximum z-score from each row as a worst-case scenario for a null run to be falsely rejected.\n",
        "  * Find the z-score threshold that would reject our desired overall Type I error rate.\n",
        "  * Convert that z-score into an equivalent individual-test error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw2tIM9LLpd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def peeking_correction(alpha = .05, p = .5, n_trials = 1000, n_blocks = 2, n_sims = 10000):\n",
        "    \"\"\"\n",
        "    This function uses simulations to estimate the individual error rate necessary\n",
        "    to limit the Type I error rate, if an early stopping decision is made based on\n",
        "    a significant result when peeking ahead.\n",
        "    \n",
        "    Input parameters:\n",
        "        alpha: Desired overall Type I error rate\n",
        "        p: Probability of individual trial success\n",
        "        n_trials: Number of trials in a full experiment\n",
        "        n_blocks: Number of times data is looked at (including end)\n",
        "        n_sims: Number of simulated experiments run\n",
        "        \n",
        "    Return:\n",
        "        alpha_ind: Individual error rate required to achieve overall error rate\n",
        "    \"\"\"\n",
        "    \n",
        "    # generate data\n",
        "    trials_per_block = np.ceil(n_trials / n_blocks).astype(int)\n",
        "    data = np.random.binomial(trials_per_block, p, [n_sims, n_blocks])\n",
        "    \n",
        "    # standardize data\n",
        "    data_cumsum = np.cumsum(data, axis = 1)\n",
        "    block_sizes = trials_per_block * np.arange(1, n_blocks+1, 1)\n",
        "    block_means = block_sizes * p\n",
        "    block_sds   = np.sqrt(block_sizes * p * (1-p))\n",
        "    data_zscores = (data_cumsum - block_means) / block_sds\n",
        "    \n",
        "    # find necessary individual error rate\n",
        "    max_zscores = np.abs(data_zscores).max(axis = 1)\n",
        "    z_crit_ind = np.percentile(max_zscores, 100 * (1 - alpha))\n",
        "    alpha_ind = 2 * (1 - stats.norm.cdf(z_crit_ind))\n",
        "    \n",
        "    return alpha_ind\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLU14pCxL5Ni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "9dee2ca8-2d53-42fc-bab3-1a0f2211cacf"
      },
      "source": [
        "peeking_correction(n_trials = 10_000, n_sims = 100_000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02941431013863638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2LMoefIL69g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Running the function on the default parameters should give a required individual error rate of about .029. Note how this is somewhat higher than the .025 or .0253 that would have been generated from the Bonferroni and Šidák corrections, respectively. Test with a higher number of simulations and trials to get more accurate estimates, and try out different numbers of blocks to see how it changes the individual error rate needed. The results should approximately match up with the numbers given in the table in the middle of this article; note that peeking $n$ times means splitting the experiment into $n + 1$ blocks.\n",
        "\n"
      ]
    }
  ]
}